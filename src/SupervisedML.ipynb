{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0c985f",
   "metadata": {},
   "source": [
    "## Displaying Features in `merged.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bf5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.data_split.stratifiedSplit import stratified_split\n",
    "from scripts.TreatImbalance.BalancingTrainingData import hybrid_balance\n",
    "from scripts.Training.TrainEvaluate import train_and_evaluate \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9053d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with shape: (735483, 41)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/preprocessed/preprocessed_reduced_data.csv\"\n",
    "\n",
    "target_col = \"fire\"\n",
    "test_size = 0.2\n",
    "desired_minority_prop = (\n",
    "        0.30  # user-chosen: 0.30 means 30% minority in balanced training set\n",
    "    )\n",
    "balanced_train_savepath = \"../data/learningTestData/balanced_train.csv\"\n",
    "\n",
    "\n",
    "data_df = pd.read_csv(data_path)\n",
    "print(\"Loaded dataset with shape:\", data_df.shape)\n",
    "if target_col not in data_df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Target column '{target_col}' not found in CSV columns: {data_df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2eb94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = stratified_split(\n",
    "        data_df, target_col=target_col, test_size=test_size, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f12b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 588386 (minority=49559, majority=538827)\n",
      "Desired minority proportion: 0.30\n",
      "Undersampling majority from 538827 -> 411870 (fast reduction).\n",
      "After undersampling: minority=49559, majority=411870\n",
      "SMOTE will generate minority to reach 138429 samples (ratio=2.79).\n",
      "Final balanced sizes: {0: 411870, 1: 138429} | total=550299\n",
      "Balanced training data saved to: ../data/learningTestData/balanced_train.csv\n"
     ]
    }
   ],
   "source": [
    "balanced_train_df = hybrid_balance(\n",
    "        train_df,\n",
    "        target_col=target_col,\n",
    "        minority_target=1,\n",
    "        desired_minority_prop=desired_minority_prop,\n",
    "        random_state=42,\n",
    "        save_path=balanced_train_savepath,\n",
    "        verbose=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e8540",
   "metadata": {},
   "source": [
    "## Building KNN Tree From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433f41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None, counts=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value      # predicted class\n",
    "            self.counts = counts    # class distribution in leaf\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    # ============================================================\n",
    "    # Sklearn compatibility\n",
    "    # ============================================================\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"max_depth\": self.max_depth,\n",
    "            \"min_samples_split\": self.min_samples_split,\n",
    "            \"min_samples_leaf\": self.min_samples_leaf,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            setattr(self, k, v)\n",
    "        return self\n",
    "\n",
    "    # ============================================================\n",
    "    # Internal helpers\n",
    "    # ============================================================\n",
    "    def entropy(self, counts):\n",
    "        total = counts.sum()\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        p = counts / total\n",
    "        return -(p * np.log2(p + 1e-9)).sum()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        idx = np.arange(len(y))\n",
    "        self.tree_ = self._build(idx, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build(self, idx, depth):\n",
    "        y = self.y[idx]\n",
    "        counts = np.bincount(y, minlength=self.n_classes_)\n",
    "\n",
    "        # stopping conditions\n",
    "        if (\n",
    "            len(idx) < self.min_samples_split or\n",
    "            (self.max_depth is not None and depth >= self.max_depth)\n",
    "        ):\n",
    "            return self.Node(value=np.argmax(counts), counts=counts)\n",
    "\n",
    "        n_features = self.X.shape[1]\n",
    "        parent_entropy = self.entropy(counts)\n",
    "\n",
    "        best_gain = 0\n",
    "        best_feat = None\n",
    "        best_thresh = None\n",
    "        best_left = None\n",
    "        best_right = None\n",
    "\n",
    "        for f in range(n_features):\n",
    "            col = self.X[idx, f]\n",
    "            order = np.argsort(col)\n",
    "            sorted_idx = idx[order]\n",
    "            sorted_y = self.y[sorted_idx]\n",
    "            sorted_col = col[order]\n",
    "\n",
    "            left_counts = np.zeros(self.n_classes_, dtype=np.int32)\n",
    "            right_counts = np.bincount(sorted_y, minlength=self.n_classes_)\n",
    "\n",
    "            for i in range(len(idx) - 1):\n",
    "                c = sorted_y[i]\n",
    "                left_counts[c] += 1\n",
    "                right_counts[c] -= 1\n",
    "\n",
    "                if sorted_col[i] == sorted_col[i+1]:\n",
    "                    continue\n",
    "\n",
    "                left_n = i + 1\n",
    "                right_n = len(idx) - left_n\n",
    "\n",
    "                if left_n < self.min_samples_leaf or right_n < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                thresh = (sorted_col[i] + sorted_col[i+1]) / 2\n",
    "\n",
    "                gain = parent_entropy - (\n",
    "                    (left_n / len(idx)) * self.entropy(left_counts)\n",
    "                    + (right_n / len(idx)) * self.entropy(right_counts)\n",
    "                )\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feat = f\n",
    "                    best_thresh = thresh\n",
    "                    best_left = sorted_idx[:left_n]\n",
    "                    best_right = sorted_idx[left_n:]\n",
    "\n",
    "        if best_gain == 0:\n",
    "            return self.Node(value=np.argmax(counts), counts=counts)\n",
    "\n",
    "        left_node = self._build(best_left, depth+1)\n",
    "        right_node = self._build(best_right, depth+1)\n",
    "        return self.Node(feature=best_feat, threshold=best_thresh,\n",
    "                         left=left_node, right=right_node, counts=counts)\n",
    "\n",
    "    def _predict_node(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_node(x, node.left)\n",
    "        return self._predict_node(x, node.right)\n",
    "\n",
    "    # ============================================================\n",
    "    # Required sklearn prediction API\n",
    "    # ============================================================\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            node = self._predict_node(x, self.tree_)\n",
    "            preds.append(node.value)\n",
    "        return np.array(preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return class probabilities using the leaf distribution.\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        proba = []\n",
    "        for x in X:\n",
    "            node = self._predict_node(x, self.tree_)\n",
    "            counts = node.counts\n",
    "            p = counts / counts.sum()\n",
    "            proba.append(p)\n",
    "        return np.array(proba)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Return raw scores (class counts before normalization).\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        scores = []\n",
    "        for x in X:\n",
    "            node = self._predict_node(x, self.tree_)\n",
    "            scores.append(node.counts)\n",
    "        return np.array(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc208ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "ðŸš€ Training: MyKNnClassifier\n",
      "ðŸ› ï¸ Parameters: {'n_neighbors': 5, 'n_jobs': -1}\n",
      "============================\n",
      "\n",
      "ðŸ“Œ Training on full training set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m , \n\u001b[0;32m      4\u001b[0m }\n\u001b[1;32m----> 5\u001b[0m results_dt \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbalanced_train_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMyKNNClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMyKNnClassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\usthb\\M2\\DataMining\\projet\\DataMining--Project\\src\\scripts\\Training\\TrainEvaluate.py:74\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(balanced_train_df, test_df, estimator, algo_name, params, target_col, verbose)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConfusion matrix (tn, fp, fn, tp):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtn_fp_fn_tp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[1;32m---> 74\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m compute_metrics(X_test, y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# SAVE RESULTS TO CSV\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Prepare dataframe\u001b[39;00m\n",
      "File \u001b[1;32md:\\usthb\\M2\\DataMining\\projet\\DataMining--Project\\src\\scripts\\Training\\TrainEvaluate.py:44\u001b[0m, in \u001b[0;36mtrain_and_evaluate.<locals>.compute_metrics\u001b[1;34m(X, y, label)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_metrics\u001b[39m(X, y, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 44\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         y_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[7], line 86\u001b[0m, in \u001b[0;36mMyKNNClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_one_cpu(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_one_cpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m )\n",
      "File \u001b[1;32md:\\usthb\\M2\\DataMining\\projet\\DataMining--Project\\DMenv\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\usthb\\M2\\DataMining\\projet\\DataMining--Project\\DMenv\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\usthb\\M2\\DataMining\\projet\\DataMining--Project\\DMenv\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_neighbors\": 5,\n",
    "    \"n_jobs\" : -1 , \n",
    "}\n",
    "results_dt = train_and_evaluate(\n",
    "    balanced_train_df,\n",
    "    test_df,\n",
    "    estimator=MyKNNClassifier(),\n",
    "    algo_name = \"MyKNnClassifier\",\n",
    "    params = params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa199df",
   "metadata": {},
   "source": [
    "## Building Decision Tree From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73aa27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    def entropy(self, counts):\n",
    "        total = counts.sum()\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        p = counts / total\n",
    "        return -(p * np.log2(p + 1e-9)).sum()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        idx = np.arange(len(y))\n",
    "        self.tree_ = self._build(idx, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build(self, idx, depth):\n",
    "        y = self.y[idx]\n",
    "        counts = np.bincount(y, minlength=self.n_classes_)\n",
    "\n",
    "        # stopping criteria\n",
    "        if (\n",
    "            len(idx) < self.min_samples_split or\n",
    "            (self.max_depth is not None and depth >= self.max_depth)\n",
    "        ):\n",
    "            return self.Node(value=np.argmax(counts))\n",
    "\n",
    "        n_features = self.X.shape[1]\n",
    "        best_gain = 0\n",
    "        best_feat = None\n",
    "        best_thresh = None\n",
    "        best_left = None\n",
    "        best_right = None\n",
    "\n",
    "        parent_entropy = self.entropy(counts)\n",
    "\n",
    "        for f in range(n_features):\n",
    "            col = self.X[idx, f]\n",
    "            order = np.argsort(col)\n",
    "            sorted_idx = idx[order]\n",
    "            sorted_y = self.y[sorted_idx]\n",
    "            sorted_col = col[order]\n",
    "\n",
    "            # class counts on left and right\n",
    "            left_counts = np.zeros(self.n_classes_, dtype=np.int32)\n",
    "            right_counts = np.bincount(sorted_y, minlength=self.n_classes_)\n",
    "\n",
    "            for i in range(len(idx) - 1):\n",
    "                c = sorted_y[i]\n",
    "                left_counts[c] += 1\n",
    "                right_counts[c] -= 1\n",
    "\n",
    "                if sorted_col[i] == sorted_col[i + 1]:\n",
    "                    continue  # same value â†’ skip\n",
    "\n",
    "                left_n = i + 1\n",
    "                right_n = len(idx) - left_n\n",
    "\n",
    "                if left_n < self.min_samples_leaf or right_n < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                thresh = (sorted_col[i] + sorted_col[i + 1]) / 2\n",
    "                gain = parent_entropy - (\n",
    "                    left_n/len(idx) * self.entropy(left_counts)\n",
    "                    + right_n/len(idx) * self.entropy(right_counts)\n",
    "                )\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feat = f\n",
    "                    best_thresh = thresh\n",
    "                    best_left = sorted_idx[:left_n]\n",
    "                    best_right = sorted_idx[left_n:]\n",
    "\n",
    "        if best_gain == 0:\n",
    "            return self.Node(value=np.argmax(counts))\n",
    "\n",
    "        left_node = self._build(best_left, depth + 1)\n",
    "        right_node = self._build(best_right, depth + 1)\n",
    "        return self.Node(feature=best_feat, threshold=best_thresh,\n",
    "                         left=left_node, right=right_node)\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_one(x, node.left)\n",
    "        return self._predict_one(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.tree_) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c682147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "ðŸš€ Training: DecisionTreeClassifier_skLearn\n",
      "ðŸ› ï¸ Parameters: {'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'entropy'}\n",
      "============================\n",
      "\n",
      "ðŸ“Œ Training on full training set...\n",
      "\n",
      "====== ðŸ“Š TRAINING SET EVALUATION ======\n",
      "\n",
      "accuracy: 0.9892\n",
      "precision: 0.9979\n",
      "recall: 0.9590\n",
      "f1: 0.9781\n",
      "roc_auc: 0.9956\n",
      "roc_auc: 0.9956\n",
      "\n",
      "Confusion matrix (tn, fp, fn, tp):\n",
      " [411586    284   5670 132759]\n",
      "\n",
      "====== ðŸ“Š TEST SET EVALUATION ======\n",
      "\n",
      "accuracy: 0.9940\n",
      "precision: 0.9833\n",
      "recall: 0.9446\n",
      "f1: 0.9636\n",
      "roc_auc: 0.9898\n",
      "roc_auc: 0.9898\n",
      "\n",
      "Confusion matrix (tn, fp, fn, tp):\n",
      " [134508    199    686  11704]\n",
      "\n",
      "ðŸ“ Results saved to: DecisionTreeClassifier_skLearn_results_2.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 20,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"min_samples_leaf\": 5,\n",
    "    \"criterion\": \"entropy\",\n",
    "\n",
    "}\n",
    "\n",
    "results_dt = train_and_evaluate(\n",
    "    balanced_train_df,\n",
    "    test_df,\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    algo_name = \"DecisionTreeClassifier_skLearn\",\n",
    "    params = params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cddd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "ðŸš€ Training: MyDecisionTreeClassifier\n",
      "ðŸ› ï¸ Parameters: {'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'entropy'}\n",
      "============================\n",
      "\n",
      "ðŸ“Œ Training on full training set...\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": 20,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"min_samples_leaf\": 5,\n",
    "    \"criterion\": \"entropy\",\n",
    "\n",
    "}\n",
    "\n",
    "results_dt = train_and_evaluate(\n",
    "    balanced_train_df,\n",
    "    test_df,\n",
    "    estimator=MyDecisionTreeClassifier(),\n",
    "    algo_name = \"MyDecisionTreeClassifier\",\n",
    "    params = params,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMenv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
