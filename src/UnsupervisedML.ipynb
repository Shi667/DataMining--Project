{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4ea82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import (\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "import os\n",
    "import hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342d83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after sampling (k-mean): (735483, 32)\n",
      "Data shape after sampling (dbscan): (147097, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 1. Load your data\n",
    "# -----------------------------\n",
    "data_path_32 = \"../data/preprocessed/preprocessed_reduced_unsupervised_32.csv\"\n",
    "X_data_32 = pd.read_csv(data_path_32)\n",
    "\n",
    "\n",
    "\n",
    "data_path_15 = \"../data/preprocessed/preprocessed_reduced_unsupervised_15.csv\"\n",
    "X_data_15 = pd.read_csv(data_path_15)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Data shape after sampling (k-mean):\", X_data_32.shape)\n",
    "print(\"Data shape after sampling (dbscan):\", X_data_15.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "577441ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(X, labels, total_samples, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    N = len(labels)\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    for k, count in zip(unique_labels, counts):\n",
    "        cluster_idx = np.where(labels == k)[0]\n",
    "        # proportional sampling: fraction of total_samples\n",
    "        n_samples = max(int(total_samples * count / N), 1)\n",
    "\n",
    "        if len(cluster_idx) <= n_samples:\n",
    "            indices.extend(cluster_idx)\n",
    "        else:\n",
    "            indices.extend(rng.choice(cluster_idx, n_samples, replace=False))\n",
    "\n",
    "    # Convert X to NumPy if it's a DataFrame\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "\n",
    "    return X[indices], labels[indices]\n",
    "\n",
    "\n",
    "\n",
    "def run_kmeans_grid(\n",
    "    X,\n",
    "    k_values,\n",
    "    output_path: str,\n",
    "    init_methods=(\"k-means++\",),\n",
    "    metrics=(\"ch\", \"dbi\", \"wcss\", \"silhouette\"),\n",
    "    algorithm=\"kmeans\",  # \"kmeans\" | \"minibatch_kmeans\"\n",
    "    max_iter=300,\n",
    "    n_init=10,\n",
    "    batch_size=1024,  # only for minibatch_kmeans\n",
    "    random_state=42,\n",
    "    silhouette_sample_size=50_000,\n",
    "    silhouette_n_repeats=5,  # number of times to repeat silhouette evaluation\n",
    "):\n",
    "    if algorithm not in {\"kmeans\", \"minibatch_kmeans\"}:\n",
    "        raise ValueError(\"algorithm must be 'kmeans' or 'minibatch_kmeans'\")\n",
    "\n",
    "    N = X.shape[0]\n",
    "    results = []\n",
    "\n",
    "    grid = list(product(k_values, init_methods))\n",
    "\n",
    "    for K, init in tqdm(grid, desc=f\"{algorithm} Grid Search\"):\n",
    "\n",
    "        # Fit KMeans / MiniBatchKMeans\n",
    "        if algorithm == \"kmeans\":\n",
    "            model = KMeans(\n",
    "                n_clusters=K,\n",
    "                init=init,\n",
    "                max_iter=max_iter,\n",
    "                n_init=n_init,\n",
    "                random_state=random_state,\n",
    "                algorithm=\"lloyd\",\n",
    "            )\n",
    "        else:\n",
    "            model = MiniBatchKMeans(\n",
    "                n_clusters=K,\n",
    "                init=init,\n",
    "                max_iter=max_iter,\n",
    "                batch_size=batch_size,\n",
    "                n_init=n_init,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "\n",
    "        labels = model.fit_predict(X)\n",
    "\n",
    "        row = {\n",
    "            \"K\": K,\n",
    "            \"init\": init,\n",
    "            \"algorithm\": algorithm,\n",
    "        }\n",
    "\n",
    "        # -----------------------\n",
    "        # Metrics\n",
    "        # -----------------------\n",
    "        if \"ch\" in metrics:\n",
    "            row[\"CH\"] = calinski_harabasz_score(X, labels)\n",
    "\n",
    "        if \"dbi\" in metrics:\n",
    "            row[\"DBI\"] = davies_bouldin_score(X, labels)\n",
    "\n",
    "        if \"silhouette\" in metrics:\n",
    "            silhouette_scores = []\n",
    "            for i in range(silhouette_n_repeats):\n",
    "                Xs, ls = stratified_sample(\n",
    "                    X,\n",
    "                    labels,\n",
    "                    total_samples=silhouette_sample_size,\n",
    "                    random_state=random_state + i,  # different seed each time\n",
    "                )\n",
    "                silhouette_scores.append(silhouette_score(Xs, ls))\n",
    "\n",
    "            row[\"Silhouette\"] = np.mean(silhouette_scores)  # average over repeats\n",
    "            row[\"Silhouette_std\"] = np.std(silhouette_scores)  # optional: track variability\n",
    "\n",
    "        if \"wcss\" in metrics:\n",
    "            row[\"WCSS_per_point\"] = model.inertia_ / N\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6d347b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch_kmeans Grid Search: 100%|██████████| 48/48 [02:12<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "results = run_kmeans_grid(\n",
    "    X=X_data_32,\n",
    "    k_values=list(range(3, 51)),  # 3,5,7,...,49\n",
    "    output_path=\"./results/knn_metrics.csv\",\n",
    "    algorithm=\"minibatch_kmeans\",  \n",
    "    batch_size=4096,\n",
    "    metrics=(\"ch\", \"dbi\",\"wcss\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7609a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    K       init         algorithm             CH       DBI  WCSS_per_point\n",
      "0   3  k-means++  minibatch_kmeans  254988.834430  1.721380       16.684688\n",
      "1   5  k-means++  minibatch_kmeans  180414.047959  1.701579       14.263527\n",
      "2   8  k-means++  minibatch_kmeans  147410.138872  1.512309       11.756861\n",
      "3  11  k-means++  minibatch_kmeans  126335.821244  1.505054       10.413611\n",
      "4  12  k-means++  minibatch_kmeans  125704.384579  1.368123        9.811640\n",
      "5  13  k-means++  minibatch_kmeans  124980.671951  1.341546        9.296829\n",
      "6  14  k-means++  minibatch_kmeans  120858.929637  1.329219        9.021307\n",
      "7  16  k-means++  minibatch_kmeans  115956.241494  1.277816        8.403590\n",
      "8  21  k-means++  minibatch_kmeans  117611.923318  1.288624        6.730543\n",
      "9  26  k-means++  minibatch_kmeans  110334.797442  1.211595        5.948190\n"
     ]
    }
   ],
   "source": [
    "def dominates(a, b, metrics, directions):\n",
    "    \"\"\"\n",
    "    Returns True if solution a dominates solution b\n",
    "    \"\"\"\n",
    "    better_or_equal = True\n",
    "    strictly_better = False\n",
    "\n",
    "    for m in metrics:\n",
    "        if directions[m] == \"max\":\n",
    "            if a[m] < b[m]:\n",
    "                better_or_equal = False\n",
    "                break\n",
    "            elif a[m] > b[m]:\n",
    "                strictly_better = True\n",
    "\n",
    "        else:  # minimize\n",
    "            if a[m] > b[m]:\n",
    "                better_or_equal = False\n",
    "                break\n",
    "            elif a[m] < b[m]:\n",
    "                strictly_better = True\n",
    "\n",
    "    return better_or_equal and strictly_better\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "METRIC_DIRECTIONS = {\n",
    "    \"CH\": \"max\",\n",
    "    \"DBI\": \"min\",\n",
    "    \"Silhouette\": \"max\",\n",
    "    \"WCSS_per_point\": \"min\",\n",
    "}\n",
    "\n",
    "def extract_pareto_front(\n",
    "    csv_path,\n",
    "    metrics,\n",
    "    directions=METRIC_DIRECTIONS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        Path to CSV file containing grid search results\n",
    "    metrics : list[str]\n",
    "        Metrics to consider for Pareto dominance\n",
    "    directions : dict\n",
    "        Metric optimization directions (\"min\" or \"max\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pareto_df : pd.DataFrame\n",
    "        Non-dominated solutions\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # --- Safety checks ---\n",
    "    for m in metrics:\n",
    "        if m not in df.columns:\n",
    "            raise ValueError(f\"Metric '{m}' not found in CSV\")\n",
    "        if m not in directions:\n",
    "            raise ValueError(f\"No direction specified for metric '{m}'\")\n",
    "\n",
    "    pareto_mask = [True] * len(df)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if not pareto_mask[i]:\n",
    "            continue\n",
    "\n",
    "        for j in range(len(df)):\n",
    "            if i == j or not pareto_mask[j]:\n",
    "                continue\n",
    "\n",
    "            if dominates(df.iloc[j], df.iloc[i], metrics, directions):\n",
    "                pareto_mask[i] = False\n",
    "                break\n",
    "\n",
    "    pareto_df = df[pareto_mask].reset_index(drop=True)\n",
    "    return pareto_df\n",
    "\n",
    "\n",
    "\n",
    "pareto = extract_pareto_front(\n",
    "    csv_path=\"./results/knn_metrics.csv\",\n",
    "    metrics=[\"CH\", \"DBI\"],\n",
    ")\n",
    "\n",
    "print(pareto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cd6890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch_kmeans Grid Search: 100%|██████████| 9/9 [55:06<00:00, 367.38s/it]\n"
     ]
    }
   ],
   "source": [
    "results = run_kmeans_grid(\n",
    "    X=X_data_32,\n",
    "    k_values=[3,5,8,11,12,14,16,21,26],  # 3,5,7,...,49\n",
    "    n_init=10,\n",
    "    output_path=\"./results/knn_silhouette.csv\",\n",
    "    algorithm=\"minibatch_kmeans\",  \n",
    "    batch_size=4096,\n",
    "    metrics=(\"silhouette\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e36dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_hdbscan_grid(\n",
    "    X,\n",
    "    min_cluster_sizes,\n",
    "    min_samples_values,\n",
    "    output_path: str,\n",
    "    metrics=(\"ch\", \"dbi\", \"silhouette\"),\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    silhouette_sample_size=50_000,\n",
    "    silhouette_n_repeats=5,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    HDBSCAN grid search with:\n",
    "    - number of clusters\n",
    "    - number & percentage of outliers\n",
    "    - internal clustering metrics\n",
    "    - incremental CSV saving (safe for long runs)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create CSV with header if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        pd.DataFrame().to_csv(output_path, index=False)\n",
    "\n",
    "    grid = list(product(min_cluster_sizes, min_samples_values))\n",
    "\n",
    "    for min_cs, min_s in tqdm(grid, desc=\"HDBSCAN Grid Search\"):\n",
    "\n",
    "        # -----------------------\n",
    "        # Fit HDBSCAN\n",
    "        # -----------------------\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cs,\n",
    "            min_samples=min_s,\n",
    "            metric=metric,\n",
    "            cluster_selection_method=cluster_selection_method,\n",
    "        )\n",
    "\n",
    "        labels = clusterer.fit_predict(X)\n",
    "\n",
    "        N = len(labels)\n",
    "        n_noise = np.sum(labels == -1)\n",
    "        noise_ratio = n_noise / N\n",
    "\n",
    "        unique_clusters = set(labels)\n",
    "        n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
    "\n",
    "        row = {\n",
    "            \"algorithm\": \"hdbscan\",\n",
    "            \"min_cluster_size\": min_cs,\n",
    "            \"min_samples\": min_s,\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"n_noise\": n_noise,\n",
    "            \"noise_ratio\": noise_ratio,\n",
    "        }\n",
    "\n",
    "        # Remove noise for metric computation\n",
    "        mask = labels != -1\n",
    "        X_clean = X[mask]\n",
    "        labels_clean = labels[mask]\n",
    "\n",
    "        # -----------------------\n",
    "        # Metrics\n",
    "        # -----------------------\n",
    "        if len(np.unique(labels_clean)) >= 2:\n",
    "\n",
    "            if \"ch\" in metrics:\n",
    "                row[\"CH\"] = calinski_harabasz_score(X_clean, labels_clean)\n",
    "\n",
    "            if \"dbi\" in metrics:\n",
    "                row[\"DBI\"] = davies_bouldin_score(X_clean, labels_clean)\n",
    "\n",
    "            if \"silhouette\" in metrics:\n",
    "                sil_scores = []\n",
    "\n",
    "                for i in range(silhouette_n_repeats):\n",
    "                    rng = np.random.RandomState(random_state + i)\n",
    "\n",
    "                    if X_clean.shape[0] > silhouette_sample_size:\n",
    "                        idx = rng.choice(\n",
    "                            X_clean.shape[0],\n",
    "                            silhouette_sample_size,\n",
    "                            replace=False,\n",
    "                        )\n",
    "                        sil_scores.append(\n",
    "                            silhouette_score(X_clean[idx], labels_clean[idx])\n",
    "                        )\n",
    "                    else:\n",
    "                        sil_scores.append(\n",
    "                            silhouette_score(X_clean, labels_clean)\n",
    "                        )\n",
    "\n",
    "                row[\"Silhouette\"] = np.mean(sil_scores)\n",
    "                row[\"Silhouette_std\"] = np.std(sil_scores)\n",
    "\n",
    "        else:\n",
    "            # Degenerate solution: all noise or one cluster\n",
    "            row[\"CH\"] = np.nan\n",
    "            row[\"DBI\"] = np.nan\n",
    "            row[\"Silhouette\"] = np.nan\n",
    "            row[\"Silhouette_std\"] = np.nan\n",
    "\n",
    "        # -----------------------\n",
    "        # Save immediately\n",
    "        # -----------------------\n",
    "        df_row = pd.DataFrame([row])\n",
    "        df_row.to_csv(\n",
    "            output_path,\n",
    "            mode=\"a\",\n",
    "            header=not os.path.getsize(output_path),\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        # Print after each evaluation\n",
    "        print(row)\n",
    "\n",
    "    return pd.read_csv(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_hdbscan_grid(\n",
    "    X=X_data_32,\n",
    "    min_cluster_sizes=[5, 10, 20, 30, 50],\n",
    "    min_samples_values=[1, 5, 10],\n",
    "    output_path=\"hdbscan_results.csv\",\n",
    "    metrics=(\"ch\", \"dbi\", \"silhouette\"),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMenv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
